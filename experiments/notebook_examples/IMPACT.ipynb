{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# IMPACT paper experiments\n",
    "### 1. Init\n",
    "#### 1.1. Import libraries (necessary)"
   ],
   "id": "bc0774a07482a44f"
  },
  {
   "cell_type": "code",
   "id": "30c2bf011793765d",
   "metadata": {},
   "source": [
    "from numpy.lib.function_base import interp\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "from IMPACT import utils\n",
    "utils.set_seed(0)\n",
    "from IMPACT import dataset\n",
    "from IMPACT import model\n",
    "import optuna\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from importlib import reload"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6f4d550fa43938b4",
   "metadata": {},
   "source": "#### 1.2. Start tensorboard (optional)"
  },
  {
   "cell_type": "code",
   "id": "db4f414458219d16",
   "metadata": {},
   "source": [
    "from tensorboard import notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --reuse=False --logdir ../tensorboard --load_fast=false --reload_interval=1\n",
    "\n",
    "print(notebook.list())\n",
    "# access tensorboard at : http://localhost:6006"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18c248901436fb79",
   "metadata": {},
   "source": "#### 1.3. Set up the loggers (recommended)"
  },
  {
   "cell_type": "code",
   "id": "f544e590b910dcad",
   "metadata": {},
   "source": [
    "utils.setuplogger(verbose = True, log_name=\"IMPACT_postcovid\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2eecb5813a4dcd9",
   "metadata": {},
   "source": [
    "### 2. CDM Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8a6aadbcd3a357",
   "metadata": {},
   "source": "#### 2.1. Sequential version"
  },
  {
   "cell_type": "code",
   "id": "53c3976f69491bab",
   "metadata": {},
   "source": [
    "reload(utils)\n",
    "reload(model)\n",
    "reload(dataset)\n",
    "\n",
    "def load_dataset(config) :\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # read datasets\n",
    "    i_fold = 0\n",
    "    concept_map = json.load(open(f'../datasets/{config[\"dataset_name\"]}/concept_map.json', 'r'))\n",
    "    concept_map = {int(k):[int(x) for x in v] for k,v in concept_map.items()}\n",
    "    nb_modalities = torch.load(f'../datasets/{config[\"dataset_name\"]}/nb_modalities.pkl',weights_only=True)\n",
    "    metadata = json.load(open(f'../datasets/{config[\"dataset_name\"]}/metadata.json', 'r'))\n",
    "    train_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_train_quadruples_vert_{i_fold}.csv',\n",
    "                             encoding='utf-8').to_records(index=False,\n",
    "                                                          column_dtypes={'student_id': int, 'item_id': int,\"dimension_id\":int,\n",
    "                                                                         \"correct\": float,\"dimension_id\":int})\n",
    "    valid_quadruplets = pd.read_csv(f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_valid_quadruples_vert_{i_fold}.csv',\n",
    "                                 encoding='utf-8').to_records(index=False,\n",
    "                                                              column_dtypes={'student_id': int, 'item_id': int,\"dimension_id\":int,\n",
    "                                                                             \"correct\": float,\"dimension_id\":int})\n",
    "\n",
    "    train_data = dataset.LoaderDataset(train_quadruplets, concept_map, metadata, nb_modalities)\n",
    "    valid_data = dataset.LoaderDataset(valid_quadruplets, concept_map, metadata, nb_modalities)\n",
    "\n",
    "    return train_data,valid_data,concept_map,metadata\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 5e-2, log=True)\n",
    "    lambda_param = trial.suggest_float('lambda', 1e-7, 1e-5, log=True)\n",
    "    #num_responses = trial.suggest_int('num_responses', 9,13)\n",
    "\n",
    "    config['learning_rate'] = lr\n",
    "    config['lambda'] = lambda_param\n",
    "    config['num_responses'] = 12\n",
    "    \n",
    "    algo = model.IMPACT(**config)\n",
    "        \n",
    "    # Init model\n",
    "    algo.init_model(train_data, valid_data)\n",
    "\n",
    "    # train model ----\n",
    "    algo.train(train_data, valid_data)\n",
    "    \n",
    "    best_valid_metric = algo.best_valid_metric\n",
    "    \n",
    "    logging.info(\"-------Trial number : \"+str(trial.number)+\"\\nBest epoch : \"+str(algo.best_epoch)+\"\\nValues : [\"+str(best_valid_metric)+\"]\\nParams : \"+str(trial.params))\n",
    "    \n",
    "    del algo.model\n",
    "    del algo   \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "                \n",
    "    return best_valid_metric"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7f525cc9a1b247f",
   "metadata": {},
   "source": [
    "%%time\n",
    "config = utils.generate_hs_config(dataset_name='postcovid', esc = 'error', valid_metric= 'rmse',metrics = ['rmse'])\n",
    "\n",
    "utils.set_seed(config[\"seed\"])\n",
    "logging.info(config['dataset_name'])\n",
    "train_data,valid_data,concept_map,metadata = load_dataset(config)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    directions=[\"minimize\"],  # Warning : specify directions for each objective (depends on the validation metric)\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "study.optimize(objective, n_trials=100, n_jobs=1, gc_after_trial=True)\n",
    "\n",
    "# Analyze the results\n",
    "## requirements : plotly, nbformat\n",
    "pareto_trials = study.best_trials\n",
    "\n",
    "logging.info(f\"Best trial for {config['dataset_name']} : {study.best_trials}\")\n",
    "for trial in pareto_trials:\n",
    "    logging.info(f\"Trial #{trial.number}\")\n",
    "    logging.info(f\"  Metric value: {trial.values}\")\n",
    "    #logging.info(f\"  DOA: {trial.values[1]}\")\n",
    "    logging.info(f\"  Params: {trial.params}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62dd8fde44ffb96d",
   "metadata": {},
   "source": "#### 2.2. Parallelized version"
  },
  {
   "cell_type": "code",
   "id": "dba2786da58afda1",
   "metadata": {},
   "source": [
    "## Launch cluster with the following command at the root of the project :\n",
    "#ipcluster start --n=3\n",
    "#ipcluster stop"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reload(utils)\n",
    "reload(model)\n",
    "reload(dataset)\n",
    "from ipyparallel import Client\n",
    "import dill\n",
    "\n",
    "cat_absolute_path = os.path.abspath('../../')\n",
    "\n",
    "rc = Client()\n",
    "rc[:].use_dill()\n",
    "lview = rc.load_balanced_view()\n",
    "\n",
    "\n",
    "rc[:].execute(\"import sys; sys.path.append('\"+cat_absolute_path+\"')\")\n",
    "logging.info(\"sys.path.append(\"+cat_absolute_path+\")\")\n",
    "with rc[:].sync_imports():\n",
    "    import json\n",
    "    from IMPACT import utils, model, dataset\n",
    "    import logging\n",
    "    import gc\n",
    "    import torch\n",
    "\n",
    "def load_dataset(config) :\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # read datasets\n",
    "    train_data, valid_data, test_data = utils.prepare_dataset(config, i_fold=0)\n",
    "\n",
    "    return train_data,valid_data,concept_map,metadata\n",
    "\n",
    "def launch_test(trial,train_data,valid_data,config) :\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    algo = model.IMPACT(**config)\n",
    "\n",
    "    # Init model\n",
    "    algo.init_model(train_data, valid_data)\n",
    "\n",
    "    # train model ----\n",
    "    algo.train(train_data, valid_data)\n",
    "\n",
    "    best_valid_metric = algo.best_valid_metric\n",
    "\n",
    "    logging.info(\"-------Trial number : \"+str(trial.number)+\"\\nBest epoch : \"+str(algo.best_epoch)+\"\\nValues : [\"+str(best_valid_metric)+\"]\\nParams : \"+str(trial.params))\n",
    "\n",
    "    del algo.model\n",
    "    del algo\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return best_valid_metric\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 5e-2, log=True)\n",
    "    lambda_param = trial.suggest_float('lambda', 1e-7, 1e-5, log=True)\n",
    "    #num_responses = trial.suggest_int('num_responses', 11,13)\n",
    "\n",
    "    config['learning_rate'] = lr\n",
    "    config['lambda'] = lambda_param\n",
    "    config['num_responses'] =12\n",
    "\n",
    "    return lview.apply_async(launch_test,trial,train_data,valid_data, config).get()\n",
    "\n"
   ],
   "id": "1c938ca9da1d7b10",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2903754279c264b8",
   "metadata": {},
   "source": [
    "%%time\n",
    "config = utils.generate_hs_config(dataset_name='postcovid', esc = 'error', valid_metric= 'rmse',metrics = ['rmse'])\n",
    "\n",
    "utils.set_seed(config[\"seed\"])\n",
    "logging.info(config['dataset_name'])\n",
    "train_data,valid_data,concept_map,metadata = load_dataset(config)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    directions=[\"minimize\"], # Warning : specify directions for each objective (depends on the validation metric)\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "study.optimize(objective, n_trials=100, n_jobs=3, gc_after_trial=True)\n",
    "\n",
    "# Analyze the results\n",
    "## requirements : plotly, nbformat\n",
    "pareto_trials = study.best_trials\n",
    "\n",
    "logging.info(f\"Best trial for {config['dataset_name']} : {study.best_trials}\")\n",
    "for trial in pareto_trials:\n",
    "    logging.info(f\"Trial #{trial.number}\")\n",
    "    logging.info(f\"  Metric value: {trial.values}\")\n",
    "    #logging.info(f\"  DOA: {trial.values[1]}\")\n",
    "    logging.info(f\"  Params: {trial.params}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da61a9e00a0211e",
   "metadata": {},
   "source": [
    "#### 2.3. Number of parameters computation"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1ba9c0a898796ca",
   "metadata": {},
   "source": [
    "d_in=5\n",
    "num_responses=13\n",
    "metadata['num_item_id']*num_responses*d_in+metadata['num_user_id']*metadata['num_dimension_id']+metadata['num_dimension_id']*metadata['num_dimension_id']*d_in"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d574559bd418d77",
   "metadata": {},
   "source": "### 3. CDM Prediction"
  },
  {
   "cell_type": "markdown",
   "id": "f79e6c9c01a718e0",
   "metadata": {},
   "source": "#### 3.1. Training and testing, sequential version"
  },
  {
   "cell_type": "code",
   "id": "59be7fecafcada09",
   "metadata": {},
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "reload(utils)\n",
    "reload(model)\n",
    "reload(dataset)\n",
    "\n",
    "def test(config : dict) :\n",
    "\n",
    "    logging.info(f'#### {config[\"dataset_name\"]} ####')\n",
    "    logging.info(f'#### config : {config} ####')\n",
    "    config['embs_path']='../embs/'+str(config[\"dataset_name\"])\n",
    "    config['params_path']='../ckpt/'+str(config[\"dataset_name\"])\n",
    "\n",
    "    pred_metrics = {m:[] for m in config['pred_metrics']}\n",
    "    profile_metrics = {m:[] for m in config['profile_metrics']}\n",
    "\n",
    "    for i_fold in range(5):\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Dataset downloading for doa and rm\n",
    "        warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in divide\")\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        train_data, valid_data, test_data = utils.prepare_dataset(config, i_fold=i_fold)\n",
    "\n",
    "        for seed in range(3):\n",
    "    \n",
    "            # Set the seed\n",
    "            utils.set_seed(seed)\n",
    "            config['seed'] = seed\n",
    "\n",
    "            algo = model.IMPACT(**config)\n",
    "\n",
    "            # Init model\n",
    "            algo.init_model(train_data, valid_data)\n",
    "\n",
    "            # train model ----\n",
    "            algo.train(train_data, valid_data)\n",
    "\n",
    "            # test model ----\n",
    "\n",
    "            eval = algo.evaluate_predictions(test_data)\n",
    "            [pred_metrics[m].append(eval[m]) for m in pred_metrics.keys()]\n",
    "\n",
    "            emb = algo.model.users_emb.weight.detach().cpu().numpy()\n",
    "            eval = algo.evaluate_profiles(test_data)\n",
    "            [profile_metrics[m].append(eval[m]) for m in profile_metrics.keys()]\n",
    "            pd.DataFrame(emb).to_csv(\"../embs/\"+config[\"dataset_name\"]+\"_IMPACT_cornac_Iter_fold\"+str(i_fold)+\"_seed_\"+str(seed)+\".csv\",index=False,header=False)\n",
    "\n",
    "\n",
    "    df_pred = pd.DataFrame(pred_metrics)\n",
    "    for m in pred_metrics.keys():\n",
    "        logging.info(f'{m} : {df_pred[m].mean()} +- {df_pred[m].std()}')\n",
    "\n",
    "    df_interp = pd.DataFrame(profile_metrics)\n",
    "    for m in profile_metrics.keys():\n",
    "        logging.info(f'{m} : {df_interp[m].mean()} +- {df_interp[m].std()}')\n",
    "\n",
    "    return df_pred,df_interp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cbcf5bc2af9449e6",
   "metadata": {},
   "source": [
    "%%time\n",
    "config = utils.generate_eval_config(esc = 'error', valid_metric= 'rmse', pred_metrics = ['rmse', 'mae'], profile_metrics = ['doa', 'pc-er', 'rm'], save_params=False)\n",
    "utils.set_seed(config[\"seed\"])\n",
    "\n",
    "config[\"dataset_name\"] = \"postcovid\"\n",
    "logging.info(config[\"dataset_name\"])\n",
    "config['learning_rate'] = 0.02026\n",
    "config['lambda'] = 1.2e-5\n",
    "config['d_in'] = 4\n",
    "config['num_responses'] = 12\n",
    "pred_metrics,df_interp = test(config)\n",
    "\n",
    "config[\"dataset_name\"] = \"assist0910\"\n",
    "logging.info(config[\"dataset_name\"])\n",
    "config['learning_rate'] = 0.02705\n",
    "config['lambda'] = 1.0e-6\n",
    "config['num_responses'] = 12\n",
    "pred_metrics,df_interp = test(config)\n",
    "\n",
    "config[\"dataset_name\"] = \"movielens\"\n",
    "logging.info(config[\"dataset_name\"])\n",
    "config['learning_rate'] = 0.02515\n",
    "config['lambda'] = 2e-7\n",
    "config['d_in'] = 10\n",
    "config['num_responses'] = 12\n",
    "pred_metrics,df_interp = test(config)\n",
    "\n",
    "config[\"dataset_name\"] = \"portrait\"\n",
    "logging.info(config[\"dataset_name\"])\n",
    "config['learning_rate'] = 0.04568\n",
    "config['lambda'] = 2e-7\n",
    "config['d_in'] = 6\n",
    "config['num_responses'] = 12\n",
    "pred_metrics,df_interp = test(config)\n",
    "\n",
    "config[\"dataset_name\"] = \"promis\"\n",
    "logging.info(config[\"dataset_name\"])\n",
    "config['learning_rate'] = 0.01227\n",
    "config['lambda'] = 1e-7\n",
    "config['d_in'] = 6\n",
    "config['num_responses'] = 13\n",
    "pred_metrics,df_interp = test(config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.2. Training and testing, parallel version",
   "id": "30e63f938bdf4a8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Launch cluster at the root of project the with the following commands :\n",
    "# ipcluster start --n=3\n",
    "# ipcluster stop"
   ],
   "id": "303c944f807c2383",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "reload(utils)\n",
    "reload(model)\n",
    "reload(dataset)\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from ipyparallel import Client\n",
    "from IMPACT import utils\n",
    "import dill\n",
    "\n",
    "cat_absolute_path = os.path.abspath('../../')\n",
    "\n",
    "rc = Client()\n",
    "rc[:].use_dill()\n",
    "lview = rc.load_balanced_view()\n",
    "\n",
    "# Synchronize imports with all engines:\n",
    "rc[:].execute(\"import sys; sys.path.append('\"+cat_absolute_path+\"')\")\n",
    "rc[:].execute(\"import os; os.chdir('./experiments/notebook_examples')\")\n",
    "\n",
    "with rc[:].sync_imports():\n",
    "    from IMPACT import utils, model, dataset\n",
    "\n",
    "def launch_training(seed:int,config:dict,train_data,valid_data,test_data) :\n",
    "    utils.set_seed(seed)\n",
    "    config['seed'] = seed\n",
    "\n",
    "    algo = model.IMPACT(**config)\n",
    "\n",
    "    # Init model\n",
    "    algo.init_model(train_data, valid_data)\n",
    "\n",
    "    # train model ----\n",
    "    algo.train(train_data, valid_data)\n",
    "\n",
    "    pred_metrics = {m:[] for m in config['pred_metrics']}\n",
    "    profile_metrics = {m:[] for m in config['profile_metrics']}\n",
    "\n",
    "    eval = algo.evaluate_predictions(test_data)\n",
    "    [pred_metrics[m].append(eval[m]) for m in pred_metrics.keys()]\n",
    "\n",
    "    emb = algo.model.users_emb.weight.detach().cpu().numpy()\n",
    "    eval = algo.evaluate_profiles(test_data)\n",
    "    [profile_metrics[m].append(eval[m]) for m in profile_metrics.keys()]\n",
    "\n",
    "    return (pred_metrics,profile_metrics,emb)\n",
    "\n",
    "def fold_test(i_fold : int, config : dict) :\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Dataset downloading for doa and rm\n",
    "    warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in divide\")\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    train_data, valid_data, test_data = utils.prepare_dataset(config, i_fold=i_fold)\n",
    "\n",
    "    pred_metrics = {m:[] for m in config['pred_metrics']}\n",
    "    profile_metrics = {m:[] for m in config['profile_metrics']}\n",
    "\n",
    "    seeds_combinations = []\n",
    "    for seed in range(3) :\n",
    "        seeds_combinations.append((seed,lview.apply_async(launch_training,seed,config,train_data,valid_data,test_data)))\n",
    "\n",
    "    for seed,async_result in seeds_combinations:\n",
    "        pred_metric, profile_metric, emb = async_result.get()\n",
    "        # test model ----\n",
    "        logging.info(f\"Test done - seed : {seed}, i_fold : {i_fold}\")\n",
    "        for k in pred_metrics.keys():\n",
    "            pred_metrics[k].extend(pred_metric[k])\n",
    "\n",
    "        for k in profile_metrics.keys():\n",
    "            profile_metrics[k].extend(profile_metric[k])\n",
    "\n",
    "        pd.DataFrame(emb).to_csv(\"../embs/\"+config[\"dataset_name\"]+\"_IMPACT_cornac_Iter_fold\"+str(i_fold)+\"_seed_\"+str(seed)+\".csv\",index=False,header=False)\n",
    "\n",
    "    return pred_metrics,profile_metrics\n",
    "\n",
    "\n",
    "def test(config : dict) :\n",
    "\n",
    "    logging.info(f'#### {config[\"dataset_name\"]} ####')\n",
    "    logging.info(f'#### config : {config} ####')\n",
    "\n",
    "    pred_metrics = {m:[] for m in config['pred_metrics']}\n",
    "    profile_metrics = {m:[] for m in config['profile_metrics']}\n",
    "\n",
    "    fold_combinations = []\n",
    "    for i_fold in range(5):\n",
    "        fold_combinations.append(fold_test(i_fold,config))\n",
    "\n",
    "    for pred_metrics_i,profile_metrics in fold_combinations:\n",
    "        for k in pred_metrics.keys():\n",
    "            pred_metrics[k].extend(pred_metrics_i[k])\n",
    "        for k in profile_metrics.keys():\n",
    "            profile_metrics[k].extend(profile_metrics[k])\n",
    "\n",
    "    df_pred = pd.DataFrame(pred_metrics)\n",
    "    for m in pred_metrics.keys():\n",
    "        logging.info(f'{m} : {df_pred[m].mean()} +- {df_pred[m].std()}')\n",
    "\n",
    "    df_interp = pd.DataFrame(profile_metrics)\n",
    "    for m in profile_metrics.keys():\n",
    "        logging.info(f'{m} : {df_interp[m].mean()} +- {df_interp[m].std()}')\n",
    "\n",
    "    return pred_metrics,df_interp"
   ],
   "id": "d8a30ea4ddf5e7b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "config = utils.generate_eval_config(esc = 'error', valid_metric= 'rmse', pred_metrics = ['rmse', 'mae'], profile_metrics = ['doa', 'pc-er', 'rm'], save_params=False)\n",
    "utils.set_seed(config[\"seed\"])\n",
    "\n",
    "config[\"dataset_name\"] = \"postcovid\"\n",
    "logging.info(config[\"dataset_name\"])\n",
    "config['learning_rate'] = 0.02026\n",
    "config['lambda'] = 1.2e-5\n",
    "config['d_in'] = 4\n",
    "config['num_responses'] = 12\n",
    "pred_metrics,df_interp = test(config)\n",
    "\n",
    "config[\"dataset_name\"] = \"assist0910\"\n",
    "logging.info(config[\"dataset_name\"])\n",
    "config['learning_rate'] = 0.02705\n",
    "config['lambda'] = 1.0e-6\n",
    "config['num_responses'] = 12\n",
    "pred_metrics,df_interp = test(config)\n",
    "\n",
    "config[\"dataset_name\"] = \"movielens\"\n",
    "logging.info(config[\"dataset_name\"])\n",
    "config['learning_rate'] = 0.02515\n",
    "config['lambda'] = 2e-7\n",
    "config['d_in'] = 10\n",
    "config['num_responses'] = 12\n",
    "pred_metrics,df_interp = test(config)\n",
    "\n",
    "config[\"dataset_name\"] = \"portrait\"\n",
    "logging.info(config[\"dataset_name\"])\n",
    "config['learning_rate'] = 0.04568\n",
    "config['lambda'] = 2e-7\n",
    "config['d_in'] = 6\n",
    "config['num_responses'] = 12\n",
    "pred_metrics,df_interp = test(config)\n",
    "\n",
    "config[\"dataset_name\"] = \"promis\"\n",
    "logging.info(config[\"dataset_name\"])\n",
    "config['learning_rate'] = 0.01227\n",
    "config['lambda'] = 1e-7\n",
    "config['d_in'] = 6\n",
    "config['num_responses'] = 13\n",
    "pred_metrics,df_interp = test(config)"
   ],
   "id": "833f5448af00d8da",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
