{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9e923c5a6fbe3b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# DeepMTP experiments\n",
    "### 1. Init\n",
    "#### 1.1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "01af8a76-5372-4abb-b7d9-80395a73d725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T11:13:20.512576Z",
     "start_time": "2025-03-12T11:13:19.023837Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "from IMPACT import utils\n",
    "utils.set_seed(0)\n",
    "from IMPACT import dataset\n",
    "from IMPACT import model\n",
    "from experiments.datasets.data_utils import DeepMTP_util\n",
    "\n",
    "import optuna\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import logging\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DeepMTP'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIMPACT\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dataset\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIMPACT\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m model\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexperiments\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DeepMTP_util\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01moptuna\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgc\u001B[39;00m\n",
      "File \u001B[0;32m~/Programmation/IMPACT/experiments/datasets/data_utils/DeepMTP_util.py:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mDeepMTP\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m generate_config\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m r2_score\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'DeepMTP'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "5b64c0ba4b3d941a",
   "metadata": {},
   "source": [
    "#### 1.2. Set up the loggers"
   ]
  },
  {
   "cell_type": "code",
   "id": "774c46a0b619fc02",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-01-13T10:53:42.760957Z",
     "start_time": "2025-01-13T10:53:42.733553Z"
    }
   },
   "source": "utils.setuplogger(verbose = True, log_name=\"DeepMTP\")",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "da32ec3e00f62e0",
   "metadata": {},
   "source": [
    "#### 1.4. Parametrize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c9c72588f9c3af8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2025-01-13T10:13:40.193478Z",
     "start_time": "2025-01-13T10:13:40.164473Z"
    }
   },
   "source": [
    "# choose dataset here\n",
    "dataset_name = 'postcovid'\n",
    "version= \"\"#\"_small\"\n",
    "# modify config here\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "config = {\n",
    "\n",
    "    # General params\n",
    "    'seed' : 0,\n",
    "\n",
    "    # Saving params\n",
    "    'load_params': False,\n",
    "    'save_params': False,\n",
    "    'embs_path' : '../embs/'+str(dataset_name),\n",
    "    'params_path' :'../ckpt/'+str(dataset_name),\n",
    "\n",
    "    # training mode\n",
    "    'early_stopping' : True,\n",
    "    'fast_training' : True, # (Only taken in account if early_stopping == true) If true, doesn't compute valid rmse PC-ER\n",
    "\n",
    "    # Learning params\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 2048,\n",
    "    'num_epochs': 200,\n",
    "    'num_dim': 10, # for IRT or MIRT todo : is it necessary as we use concepts knowledge number as embedding dimension ?\n",
    "    'eval_freq' : 1,\n",
    "    'patience' : 30,\n",
    "    'device': device,\n",
    "    'lambda' : 7.7e-6,\n",
    "    'tensorboard': False,\n",
    "    'flush_freq' : True,\n",
    "    \n",
    "    # for NeuralCD\n",
    "    'prednet_len1': 128,\n",
    "    'prednet_len2': 64,\n",
    "    'best_params_path':'',\n",
    "    \n",
    "    #For GCCD\n",
    "    'num_layers': 0,\n",
    "    'version': 'pair',\n",
    "    'p_dropout': 0,\n",
    "    'low_mem_mode' : True,\n",
    "    'user_nbrs_n' : 10,\n",
    "    'item_nbrs_n' : 5\n",
    "}\n",
    "concept_map = json.load(open(f'../datasets/{dataset_name}/concept_map.json', 'r'))\n",
    "concept_map = {int(k):[int(x) for x in v] for k,v in concept_map.items()}\n",
    "metadata = json.load(open(f'../datasets/{dataset_name}/metadata.json', 'r'))\n",
    "utils.set_seed(config['seed'])\n",
    "dataset_name += version\n",
    "logging.info(f'#### {dataset_name} ####')\n",
    "logging.info(f'#### config : {config} ####')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 13:40] #### postcovid ####\n",
      "[INFO 13:40] #### config : {'seed': 0, 'load_params': False, 'save_params': False, 'embs_path': '../embs/postcovid', 'params_path': '../ckpt/postcovid', 'early_stopping': True, 'fast_training': True, 'learning_rate': 0.001, 'batch_size': 2048, 'num_epochs': 200, 'num_dim': 10, 'eval_freq': 1, 'patience': 30, 'device': 'cuda:0', 'lambda': 7.7e-06, 'tensorboard': False, 'flush_freq': True, 'prednet_len1': 128, 'prednet_len2': 64, 'best_params_path': '', 'num_layers': 0, 'version': 'pair', 'p_dropout': 0, 'low_mem_mode': True, 'user_nbrs_n': 10, 'item_nbrs_n': 5} ####\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ea3179426d9afedb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2. CDM Training\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f2a31713158f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T10:13:43.152519Z",
     "start_time": "2025-01-13T10:13:43.128632Z"
    }
   },
   "source": [
    "reload(utils)\n",
    "reload(model)\n",
    "reload(dataset)\n",
    "\n",
    "seed = 0\n",
    "utils.set_seed(0)\n",
    "\n",
    "config['seed'] = seed\n",
    "config['early_stopping'] = True\n",
    "config['esc'] = 'objectives' #'loss' 'delta_objectives'\n",
    "config['num_epochs']=10\n",
    "config['eval_freq']=1\n",
    "config['patience']=30\n",
    "\n",
    "config['verbose_early_stopping'] = False\n",
    "config[\"tensorboard\"] = False\n",
    "config['flush_freq'] = False\n",
    "config['save_params']= False\n",
    "config['disable_tqdm'] = True"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "reload(DeepMTP_util)\n",
    "\n",
    "dataset_name = \"postcovid\"\n",
    "data,metadata = DeepMTP_util.load_dataset(dataset_name)\n",
    "train,val,test,data_info = data\n",
    "\n",
    "study = optuna.create_study(\n",
    "    directions=[\"minimize\"],  # Specify directions for each objective\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "study.optimize(lambda trial: DeepMTP_util.objective_MTP(trial, config, metadata, DeepMTP_util.generate_DeepMTP,train,val,test), n_trials=100, timeout=1800, n_jobs=1, gc_after_trial=True)\n",
    "\n",
    "# Analyze the results\n",
    "## requirements : plotly, nbformat\n",
    "pareto_trials = study.best_trials\n",
    "logging.info(f\"Best trial for {dataset_name} : {study.best_trials}\")\n",
    "\n",
    "logging.info(\"Number of trials :\"+str(len(study.trials)))\n",
    "for trial in study.trials:\n",
    "    logging.info(f\"Trial #{trial.number}\")\n",
    "    logging.info(f\"  RMSE: {trial.values}\")\n",
    "    #logging.info(f\"  DOA: {trial.values[1]}\")\n",
    "    logging.info(f\"  Params: {trial.params}\")\n",
    "\n",
    "dataset_name = \"promis\"\n",
    "data,metadata = DeepMTP_util.load_dataset(dataset_name)\n",
    "train,val,test,data_info = data\n",
    "\n",
    "study = optuna.create_study(\n",
    "    directions=[\"minimize\"],  # Specify directions for each objective\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "study.optimize(lambda trial: DeepMTP_util.objective_MTP(trial, config, metadata, DeepMTP_util.generate_DeepMTP,train,val,test), n_trials=100, timeout=14400, n_jobs=4, gc_after_trial=True)\n",
    "\n",
    "# Analyze the results\n",
    "## requirements : plotly, nbformat\n",
    "pareto_trials = study.best_trials\n",
    "logging.info(f\"Best trial for {dataset_name} : {study.best_trials}\")\n",
    "\n",
    "logging.info(\"Number of trials :\"+str(len(study.trials)))\n",
    "for trial in study.trials:\n",
    "    logging.info(f\"Trial #{trial.number}\")\n",
    "    logging.info(f\"  RMSE: {trial.values}\")\n",
    "    #logging.info(f\"  DOA: {trial.values[1]}\")\n",
    "    logging.info(f\"  Params: {trial.params}\")\n",
    "\n",
    "dataset_name = \"movielens\"\n",
    "data,metadata = DeepMTP_util.load_dataset(dataset_name)\n",
    "train,val,test,data_info = data\n",
    "\n",
    "study = optuna.create_study(\n",
    "    directions=[\"minimize\"],  # Specify directions for each objective\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "study.optimize(lambda trial: DeepMTP_util.objective_MTP(trial, config, metadata, DeepMTP_util.generate_DeepMTP,train,val,test), n_trials=100, timeout=14400, n_jobs=4, gc_after_trial=True)\n",
    "\n",
    "# Analyze the results\n",
    "## requirements : plotly, nbformat\n",
    "pareto_trials = study.best_trials\n",
    "logging.info(f\"Best trial for {dataset_name} : {study.best_trials}\")\n",
    "\n",
    "logging.info(\"Number of trials :\"+str(len(study.trials)))\n",
    "for trial in study.trials:\n",
    "    logging.info(f\"Trial #{trial.number}\")\n",
    "    logging.info(f\"  RMSE: {trial.values}\")\n",
    "    #logging.info(f\"  DOA: {trial.values[1]}\")\n",
    "    logging.info(f\"  Params: {trial.params}\")\n",
    "\n",
    "dataset_name = \"portrait\"\n",
    "data,metadata = DeepMTP_util.load_dataset(dataset_name)\n",
    "train,val,test,data_info = data\n",
    "\n",
    "study = optuna.create_study(\n",
    "    directions=[\"minimize\"],  # Specify directions for each objective\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "study.optimize(lambda trial: DeepMTP_util.objective_MTP(trial, config, metadata, DeepMTP_util.generate_DeepMTP,train,val,test), n_trials=100, timeout=14400, n_jobs=4, gc_after_trial=True)\n",
    "\n",
    "# Analyze the results\n",
    "## requirements : plotly, nbformat\n",
    "pareto_trials = study.best_trials\n",
    "logging.info(f\"Best trial for {dataset_name} : {study.best_trials}\")\n",
    "\n",
    "logging.info(\"Number of trials :\"+str(len(study.trials)))\n",
    "for trial in study.trials:\n",
    "    logging.info(f\"Trial #{trial.number}\")\n",
    "    logging.info(f\"  RMSE: {trial.values}\")\n",
    "    #logging.info(f\"  DOA: {trial.values[1]}\")\n",
    "    logging.info(f\"  Params: {trial.params}\")\n",
    "\n",
    "dataset_name = \"postcovid\"\n",
    "data,metadata = DeepMTP_util.load_dataset(dataset_name)\n",
    "train,val,test,data_info = data\n",
    "\n",
    "study = optuna.create_study(\n",
    "    directions=[\"minimize\"],  # Specify directions for each objective\n",
    ")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "study.optimize(lambda trial: DeepMTP_util.objective_MTP(trial, config, metadata, DeepMTP_util.generate_DeepMTP,train,val,test), n_trials=1, timeout=1800, n_jobs=4, gc_after_trial=True)\n",
    "\n",
    "# Analyze the results\n",
    "## requirements : plotly, nbformat\n",
    "pareto_trials = study.best_trials\n",
    "logging.info(f\"Best trial for {dataset_name} : {study.best_trials}\")\n",
    "\n",
    "logging.info(\"Number of trials :\"+str(len(study.trials)))\n",
    "for trial in study.trials:\n",
    "    logging.info(f\"Trial #{trial.number}\")\n",
    "    logging.info(f\"  RMSE: {trial.values}\")\n",
    "    #logging.info(f\"  DOA: {trial.values[1]}\")\n",
    "    logging.info(f\"  Params: {trial.params}\")"
   ],
   "id": "9cd70bc1ad551adb"
  },
  {
   "cell_type": "markdown",
   "id": "c548e520-45cf-485d-bac5-e583c2406757",
   "metadata": {},
   "source": [
    "### 3. CDM Prediction\n",
    "#### 3.1. Training and testing"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T13:02:44.558346Z",
     "start_time": "2025-01-13T12:59:51.796407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reload(DeepMTP_util)\n",
    "\n",
    "\n",
    "dataset_name = \"postcovid\"\n",
    "logging.info(dataset_name)\n",
    "config['learning_rate'] = 0.00833\n",
    "config['lambda'] = 1e-7\n",
    "metrics = DeepMTP_util.test(dataset_name,config)\n",
    "\n",
    "#\n",
    "# dataset_name = \"promis\"\n",
    "# logging.info(dataset_name)\n",
    "# config['learning_rate'] = 0.00027\n",
    "# config['lambda'] = 3e-7\n",
    "# metrics = DeepMTP_util.test(dataset_name,config)\n",
    "#\n",
    "# dataset_name = \"movielens\"\n",
    "# logging.info(dataset_name)\n",
    "# config['learning_rate'] = 0.00153\n",
    "# config['lambda'] = 5e-7\n",
    "# metrics = DeepMTP_util.test(dataset_name,config)\n",
    "#\n",
    "# dataset_name = \"portrait\"\n",
    "# logging.info(dataset_name)\n",
    "# config['learning_rate'] = 0.00735\n",
    "# config['lambda'] = 1e-7\n",
    "# metrics = DeepMTP_util.test(dataset_name,config)"
   ],
   "id": "464c84fda463eae5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO 59:51] postcovid\n",
      "Interaction file: triplet format detected\n",
      "Interaction file: triplet format detected\n",
      "Interaction file: triplet format detected\n",
      "Interaction file: checking format consistency... Passed\n",
      "Interaction file: checking instance id format consistency... Passed\n",
      "Interaction file: checking target id type consistency... Passed\n",
      "\n",
      "Interaction file: checking target variable type consistency... Passed\n",
      "Automatically detected type of target variable type: real-valued\n",
      "\n",
      "Interaction file: Checking for novel instances... Done\n",
      "-- no Novel instances detected in the test set\n",
      "Interaction file: Checking for novel targets... Done\n",
      "-- no Novel targets detected in the test set\n",
      "Estimating validation setting... Done-- Detected as setting :A\n",
      "\n",
      "Instance features file: processing features... Done\n",
      "Instance features file: processing features... Done\n",
      "Instance features file: processing features... Done\n",
      "Instance features file: processing features... Done\n",
      "Instance features file: processing features... Done\n",
      "Instance features file: processing features... Done\n",
      "\n",
      "-- Same instance ids in the interaction and features files for the train set\n",
      "-- Same instance ids in the interaction and features files for the test set\n",
      "-- Same instance ids in the interaction and features files for the val set\n",
      "-- Same target ids in the interaction and features files for the train set\n",
      "-- Same target ids in the interaction and features files for the test set\n",
      "-- Same target ids in the interaction and features files for the val set\n",
      "\n",
      "Warning: instance_branch_layers is a necessary hyperparameter to define the instance branch. Using None as the default\n",
      "Warning: instance_branch_nodes_reducing_factor is a necessary hyperparameter to define the instance branch. Using 2 as the default\n",
      "Warning: target_branch_layers is a necessary hyperparameter to define the target branch. Using None as the default\n",
      "Warning: target_branch_nodes_reducing_factor is a necessary hyperparameter to define the target branch. Using 2 as the default\n",
      "Warning: instance_branch_layers is a necessary hyperparameter to define the instance branch. Using None as the default\n",
      "Warning: instance_branch_nodes_reducing_factor is a necessary hyperparameter to define the instance branch. Using 2 as the default\n",
      "Warning: target_branch_layers is a necessary hyperparameter to define the target branch. Using None as the default\n",
      "Warning: target_branch_nodes_reducing_factor is a necessary hyperparameter to define the target branch. Using 2 as the default\n",
      "Warning: instance_branch_layers is a necessary hyperparameter to define the instance branch. Using None as the default\n",
      "Warning: instance_branch_nodes_reducing_factor is a necessary hyperparameter to define the instance branch. Using 2 as the default\n",
      "Warning: target_branch_layers is a necessary hyperparameter to define the target branch. Using None as the default\n",
      "Warning: target_branch_nodes_reducing_factor is a necessary hyperparameter to define the target branch. Using 2 as the default\n",
      "Interaction file: triplet format detected\n",
      "Interaction file: triplet format detected\n",
      "Interaction file: triplet format detected\n",
      "Interaction file: checking format consistency... Passed\n",
      "Interaction file: checking instance id format consistency... Passed\n",
      "Interaction file: checking target id type consistency... Passed\n",
      "\n",
      "Interaction file: checking target variable type consistency... Passed\n",
      "Automatically detected type of target variable type: real-valued\n",
      "\n",
      "Interaction file: Checking for novel instances... Done\n",
      "-- no Novel instances detected in the test set\n",
      "Interaction file: Checking for novel targets... Done\n",
      "-- no Novel targets detected in the test set\n",
      "Estimating validation setting... Done-- Detected as setting :A\n",
      "\n",
      "Instance features file: processing features... Done\n",
      "Instance features file: processing features... Done\n",
      "Instance features file: processing features... Done\n",
      "Instance features file: processing features... Done\n",
      "Instance features file: processing features... Done\n",
      "Instance features file: processing features... Done\n",
      "\n",
      "-- Same instance ids in the interaction and features files for the train set\n",
      "-- Same instance ids in the interaction and features files for the test set\n",
      "-- Same instance ids in the interaction and features files for the val set\n",
      "-- Same target ids in the interaction and features files for the train set\n",
      "-- Same target ids in the interaction and features files for the test set\n",
      "-- Same target ids in the interaction and features files for the val set\n",
      "\n",
      "Warning: instance_branch_layers is a necessary hyperparameter to define the instance branch. Using None as the default\n",
      "Warning: instance_branch_nodes_reducing_factor is a necessary hyperparameter to define the instance branch. Using 2 as the default\n",
      "Warning: target_branch_layers is a necessary hyperparameter to define the target branch. Using None as the default\n",
      "Warning: target_branch_nodes_reducing_factor is a necessary hyperparameter to define the target branch. Using 2 as the default\n",
      "Warning: instance_branch_layers is a necessary hyperparameter to define the instance branch. Using None as the default\n",
      "Warning: instance_branch_nodes_reducing_factor is a necessary hyperparameter to define the instance branch. Using 2 as the default\n",
      "Warning: target_branch_layers is a necessary hyperparameter to define the target branch. Using None as the default\n",
      "Warning: target_branch_nodes_reducing_factor is a necessary hyperparameter to define the target branch. Using 2 as the default\n",
      "Warning: instance_branch_layers is a necessary hyperparameter to define the instance branch. Using None as the default\n",
      "Warning: instance_branch_nodes_reducing_factor is a necessary hyperparameter to define the instance branch. Using 2 as the default\n",
      "Warning: target_branch_layers is a necessary hyperparameter to define the target branch. Using None as the default\n",
      "Warning: target_branch_nodes_reducing_factor is a necessary hyperparameter to define the target branch. Using 2 as the default\n",
      "{'mae': [0.23150551, 0.22821844, 0.22919585, 0.22557268, 0.22459361, 0.22517437], 'rmse': [0.28405493, 0.28217104, 0.28409553, 0.27975264, 0.27977636, 0.2793504], 'pc-er': [0.05012464313103243, -0.04961388492789333, -0.11057769654668287, 0.04988883284086688, -0.11636257277000155, -0.18113740106463802], 'doa': [0.4935791664237622, 0.4546267026111233, 0.44909210275863976, 0.5062204302005026, 0.44695299068374295, 0.4313775574318687], 'rm': [0.02105229559211969, -0.11525578763568817, -0.1869283845401986, 0.04462191314950045, -0.16927036466525003, -0.2606072247413256]}\n",
      "[INFO 02:44] {'mae': [0.23150551, 0.22821844, 0.22919585, 0.22557268, 0.22459361, 0.22517437], 'rmse': [0.28405493, 0.28217104, 0.28409553, 0.27975264, 0.27977636, 0.2793504], 'pc-er': [0.05012464313103243, -0.04961388492789333, -0.11057769654668287, 0.04988883284086688, -0.11636257277000155, -0.18113740106463802], 'doa': [0.4935791664237622, 0.4546267026111233, 0.44909210275863976, 0.5062204302005026, 0.44695299068374295, 0.4313775574318687], 'rm': [0.02105229559211969, -0.11525578763568817, -0.1869283845401986, 0.04462191314950045, -0.16927036466525003, -0.2606072247413256]}\n",
      "[INFO 02:44] rmse : 0.2815 +- 0.0022\n",
      "[INFO 02:44] mae : 0.2274 +- 0.0027\n",
      "[INFO 02:44] pc-er : -0.0596 +- 0.0946\n",
      "[INFO 02:44] doa : 0.4636 +- 0.0294\n",
      "[INFO 02:44] rm : -0.1111 +- 0.1210\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:58:21.217237Z",
     "start_time": "2025-01-13T12:58:21.194494Z"
    }
   },
   "cell_type": "code",
   "source": "metrics",
   "id": "715404a13f023fe5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mae': [0.2224221],\n",
       " 'rmse': [0.27964154],\n",
       " 'pc-er': [0.03776286723207629],\n",
       " 'doa': [0.500698497767473],\n",
       " 'rm': [0.021603574001723948]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89a6e10eae4eeb11"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
